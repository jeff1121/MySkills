# CLI Interface Contract

**Purpose**: å®šç¾© K8S-Installer çš„å‘½ä»¤åˆ—ä»‹é¢

## Commands

### 1. Install Command

å•Ÿå‹• K8S å¢é›†å®‰è£æµç¨‹ã€‚

```bash
# äº’å‹•æ¨¡å¼ï¼ˆæç¤ºè¼¸å…¥ç¯€é»è³‡è¨Šï¼‰
python main.py install

# ä½¿ç”¨è¨­å®šæª”
python main.py install --config cluster.yaml
```

**Options**:
| é¸é … | å‹åˆ¥ | å¿…å¡« | èªªæ˜ |
|------|------|------|------|
| --config, -c | string | âŒ | å¢é›†è¨­å®šæª”è·¯å¾‘ |
| --dry-run | flag | âŒ | åªé©—è­‰ä¸åŸ·è¡Œ |
| --verbose, -v | flag | âŒ | é¡¯ç¤ºè©³ç´°è¼¸å‡º |

**äº’å‹•å¼æç¤ºæµç¨‹**:
```
ğŸ“¦ K8S-Installer - Kubernetes å¢é›†å®‰è£å·¥å…·

=== Control Plane ç¯€é»è¨­å®š ===
  HostAddr: 192.168.1.100
  HostPort [22]: 
  HostUser: root
  HostPass: ********

=== Worker ç¯€é»è¨­å®š ===
Worker ç¯€é»æ•¸é‡ [4]: 3

--- Worker 1 ---
  HostAddr: 192.168.1.101
  HostPort [22]: 
  HostUser: root
  HostPass: ********

[ç¹¼çºŒ Worker 2, 3...]

ç¢ºèªé–‹å§‹å®‰è£ï¼Ÿ [y/N]: y
```

**Output (Success)**:
```json
{
  "success": true,
  "message": "K8S å¢é›†å®‰è£å®Œæˆ",
  "cluster": {
    "control_plane": "192.168.1.100",
    "workers": ["192.168.1.101", "192.168.1.102", "192.168.1.103"],
    "join_command": "kubeadm join 192.168.1.100:6443 --token xxx --discovery-token-ca-cert-hash sha256:xxx"
  }
}
```

**Output (Failure)**:
```json
{
  "success": false,
  "message": "å®‰è£å¤±æ•—æ–¼æ­¥é©Ÿï¼šåˆå§‹åŒ– Control Plane",
  "error": "SSH é€£ç·šå¤±æ•—ï¼šConnection refused",
  "failed_node": "192.168.1.100"
}
```

---

### 2. Validate Command

é©—è­‰ç¯€é»é€£ç·šèˆ‡å‰ç½®æ¢ä»¶ã€‚

```bash
python main.py validate --config cluster.yaml
```

**Output**:
```json
{
  "success": true,
  "nodes": [
    {"host": "192.168.1.100", "ssh": "ok", "os": "Ubuntu 22.04"},
    {"host": "192.168.1.101", "ssh": "ok", "os": "Ubuntu 22.04"},
    {"host": "192.168.1.102", "ssh": "failed", "error": "Auth failed"}
  ]
}
```

---

### 3. Status Command

æª¢æŸ¥å·²å®‰è£å¢é›†çš„ç‹€æ…‹ã€‚

```bash
python main.py status --control-plane 192.168.1.100
```

**Output**:
```json
{
  "success": true,
  "cluster_status": "healthy",
  "nodes": [
    {"name": "master", "status": "Ready", "role": "control-plane"},
    {"name": "worker-1", "status": "Ready", "role": "worker"},
    {"name": "worker-2", "status": "Ready", "role": "worker"}
  ]
}
```

---

## Exit Codes

| Code | èªªæ˜ |
|------|------|
| 0 | æˆåŠŸ |
| 1 | ä¸€èˆ¬éŒ¯èª¤ |
| 2 | åƒæ•¸éŒ¯èª¤ |
| 3 | SSH é€£ç·šå¤±æ•— |
| 4 | å®‰è£æ­¥é©Ÿå¤±æ•— |

---

## Configuration File Format

`cluster.yaml`:
```yaml
control_plane:
  host: 192.168.1.100
  port: 22
  user: root
  password: your_password

workers:
  - host: 192.168.1.101
    port: 22
    user: root
    password: your_password
  - host: 192.168.1.102
    port: 22
    user: root
    password: your_password
  - host: 192.168.1.103
    port: 22
    user: root
    password: your_password
  - host: 192.168.1.104
    port: 22
    user: root
    password: your_password

pod_network_cidr: 10.244.0.0/16
```
